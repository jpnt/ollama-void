version: '3.8'

services:
  base-config:
    build: .
    container_name: void_air_ctr
    ports:
      - "11434:11434"
    deploy:
      resources:
        limits:
          cpus: '4.0'   # Minimum 4 CPUs
          memory: 8g    # Minimum 8 GB of RAM
    environment:
      #- SOME_ENV_VAR=value
      #volumes:
      #  - llama-cpp:/root/.void-ai-ctr

  nvidia-config:
    extends: base-config
    container_name: void_ai_ctr_nvidia
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    profiles:
      - nvidia

  amd-config:
    extends: base-config
    container_name: void_ai_ctr_amd
    #image: ollama/ollama:rocm
    devices:
      - /dev/kfd
      - /dev/dri
    profiles:
      - amd
